import numpy as np
import math

class MLP:

    """
    Constructor: Computes MLP.

    Args:
        inputLayer (int): size of input
        hiddenLayers (array-like): number of layers and size of each layers.
        outputLayer (int): size of output layer
        seed (scalar): seed of the random numeric.
        epislom (scalar) : random initialization range. e.j: 1 = [-1..1], 2 = [-2,2]...
    """

    def __init__(self,inputLayer, hiddenLayers, outputLayer, seed=0, epislom = 0.12):
        np.random.seed(seed)

        self.numLayers = len(hiddenLayers) + 2
        self.hiddenLayers = hiddenLayers
        self.inputLayer = inputLayer
        self.outputLayer = outputLayer

        #Inicializamos las thetas a random (input-hidden[0], hidden[0]-hidden[1], ..., hidden[i]-output)
        self.thetas = []
        prev_layer = inputLayer
        for layer in self.hiddenLayers:
            self.thetas.append(np.random.uniform(low = -epislom, high =  epislom, size = (layer, prev_layer +1))) #Incluye bias
            prev_layer = layer    

        self.thetas.append(np.random.uniform(low = -epislom, high = epislom, size = (outputLayer, self.hiddenLayers[len(self.hiddenLayers) - 1]  +1))) #Output layer


    """
    Reset the theta matrix created in the constructor by both theta matrix manualy loaded.

    Args:
        theta1 (array_like): Weights for the first layer in the neural network.
        theta2 (array_like): Weights for the second layer in the neural network.
    """
    def new_trained(self,theta1,theta2):
        self.theta1 = theta1
        self.theta2 = theta2
    

    """
    Num elements in the training data. (private)

    Args:
        x (array_like): input data. 
    """
    def _size(self,x):
        return x.shape[0]
    

    """
    Computes de sigmoid function of z (private)
    Args:
        z (array_like): activation signal received by the layer.
    """
    def _sigmoid(self,z):
        """
        La función sigmoide  mapea cualquier número real a un rango entre 0 y 1. 
        Sirve como función de activación ya que activa las neuronas cuando el valor
        es suficientemente grande o las desactiva cuando es pequeño
        """
        return 1 / (1 + np.exp(-z))


    """
    Computes de sigmoid derivation of de activation (private)

    Args:
        a (array_like): activation received by the layer.
    """   
    def _sigmoidPrime(self,a):
        return a * (1 - a)

    """
    Run the feedwordwar neural network step

    Args:
        x (array_like): input of the neural network.

	Return 
	------
	a1,a2,a3 (array_like): activation functions of each layers
    z2,z3 (array_like): signal fuction of two last layers
    """
    def feedforward(self,x):
        a = []
        z = []
        a.append(x)
        for i in range (0,len(self.thetas)):
            a[i] = np.hstack([np.ones((a[i].shape[0], 1)), a[i]])
            z.append(a[i] @ self.thetas[i].T)
            a.append(self._sigmoid(z[i]))

        return a, z 



    """
    Computes only the cost of a previously generated output (private)

    Args:
        yPrime (array_like): output generated by neural network.
        y (array_like): output from the dataset
        lambda_ (scalar): regularization parameter

	Return 
	------
	J (scalar): the cost.
    """
    def compute_cost(self, yPrime,y, lambda_): 
        m = y.shape[0]
        outputs = np.sum(y * np.log(yPrime) + (1-y)*np.log(1 - yPrime))
        J =  (-1/ m) * outputs
        J += self._regularizationL2Cost(m, lambda_)
        return J
    

    """
    Get the class with highest activation value

    Args:
        a3 (array_like): output generated by neural network.

	Return 
	------
	p (scalar): the class index with the highest activation value.
    """
    def predict(self,a3):
        p = np.argmax(a3, axis=1)
        return p
    

    """
    Compute the gradients of both theta matrix parámeters and cost J

    Args:
        x (array_like): input of the neural network.
        y (array_like): output of the neural network.
        lambda_ (scalar): regularization.

	Return 
	------
	J: cost
    grad1, grad2: the gradient matrix (same shape than theta1 and theta2)
    """
    def compute_gradients(self, x, y, lambda_):
        m = x.shape[0]
        a, z = self.feedforward(x)
        J = self.compute_cost(a[-1], y, lambda_) #a[-1] corresponde a las activaciones de la última capa, es decir y'

        error = []
        for i in range(self.numLayers):
            if i == 0:
                error.append(np.zeros((m, self.inputLayer)))
            elif i == self.numLayers-1:
                error.append(np.zeros((m, self.outputLayer)))
            else:
                error.append(np.zeros((m, self.hiddenLayers[i-1])))

        error[-1] = (a[-1] - y)
        error[self.numLayers-2] = (np.dot(error[self.numLayers-1],self.thetas[self.numLayers-2]) * self._sigmoidPrime(a[self.numLayers-2]))
        for l in range(self.numLayers-3, 0, -1):
            error[l] = (np.dot(error[l+1][:, 1:],self.thetas[l]) * self._sigmoidPrime(a[l]))

        grad = [np.zeros_like(theta) for theta in self.thetas]

        for l in range(len(self.hiddenLayers), -1, -1):
            if l == len(self.hiddenLayers):
                grad[l] += np.dot(error[l+1].T, a[l]) / m
                grad[l] += self._regularizationL2Gradient(self.thetas[l], lambda_, m)
            else:
                grad[l] += np.dot(error[l+1][:, 1:].T, a[l]) / m
                grad[l] += self._regularizationL2Gradient(self.thetas[l], lambda_, m)

        return (J, grad)
    
    """
    Compute L2 regularization gradient

    Args:
        theta (array_like): a theta matrix to calculate the regularization.
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 Gradient value
    """
    def _regularizationL2Gradient(self, theta, lambda_, m):
        ac = np.zeros_like(theta)
        ac[:, 1:] = (lambda_ / m) * (theta[:, 1:])
        return ac
    
    
    """
    Compute L2 regularization cost

    Args:
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 cost value
    """

    def _regularizationL2Cost(self, m, lambda_):
        coste = 0
        for i in range(len(self.thetas)):
            coste += np.sum(np.square(self.thetas[i][:, 1:]))
        return lambda_ * (1 / (2*m)) * coste
    
    
    def backpropagation(self, x, y, alpha, lambda_, numIte, verbose=0):
        Jhistory = []
        for i in range(numIte):
            #Calcula las gradientes y el coste
            J, grad = self.compute_gradients(x, y, lambda_)
            Jhistory.append(J)

            #Actualiza los pesos
            for j in range(len(self.thetas)):
                self.thetas[j] -= alpha*grad[j]

            if verbose > 0 :
                if i % verbose == 0 or i == (numIte-1):
                    print(f"Iteration {(i+1):6}: Cost {float(J):8.4f}   ")
        
        return Jhistory
    


"""
target_gradient funcitón of gradient test 1
"""
def target_gradient(input_layer_size,hidden_layer_size,num_labels,x,y,reg_param):
    hiddenLayer = [hidden_layer_size]
    mlp = MLP(input_layer_size,hiddenLayer,num_labels)
    J, grad= mlp.compute_gradients(x,y,reg_param)
    return J, grad[0], grad[1], mlp.thetas[0], mlp.thetas[1]


"""
costNN funcitón of gradient test 1
"""
def costNN(Theta1, Theta2,x, ys, reg_param):
    hiddenLayer = [1]
    mlp = MLP(x.shape[1],hiddenLayer, ys.shape[1])
    mlp.new_trained(Theta1,Theta2)
    J, grad = mlp.compute_gradients(x,ys,reg_param)
    return J, grad[0], grad[1]


"""
mlp_backprop_predict 2 to be execute test 2
"""
def MLP_backprop_predict(X_train,y_train, X_test, alpha, lambda_, num_ite, verbose):
    mlp = MLP(X_train.shape[1],[30,15],y_train.shape[1])
    Jhistory = mlp.backpropagation(X_train,y_train,alpha,lambda_,num_ite,verbose)
    a = mlp.feedforward(X_test)[0]
    a3 = a[-1]
    y_pred=mlp.predict(a3)
    return y_pred